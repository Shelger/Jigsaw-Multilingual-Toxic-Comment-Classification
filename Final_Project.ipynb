{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5932f92c-814d-40c2-80a0-3157ab77a3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61f702a2-c065-4455-80a6-11ac2af0e04c",
   "metadata": {},
   "source": [
    "Using BERT on huggingface and load them into mps device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d28e0d8-e9b7-4f8a-95bd-4bbf2ba108ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shelger/miniconda3/envs/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_labels = 6\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "model = BertForSequenceClassification(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fca84ab-8de8-4716-91cf-7de8b9ba4284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shelger/miniconda3/envs/langchain/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
      "0             0        0       0       0              0   \n",
      "1             0        0       0       0              0   \n",
      "2             0        0       0       0              0   \n",
      "3             0        0       0       0              0   \n",
      "4             0        0       0       0              0   \n",
      "\n",
      "                                      input_word_ids  \\\n",
      "0  (101, 27746, 31609, 11809, 24781, 10105, 70971...   \n",
      "1  (101, 141, 112, 56237, 10874, 106, 10357, 1825...   \n",
      "2  (101, 35936, 10817, 117, 146, 112, 181, 30181,...   \n",
      "3  (101, 107, 15946, 146, 10944, 112, 188, 13086,...   \n",
      "4  (101, 11065, 117, 52523, 117, 10301, 15127, 51...   \n",
      "\n",
      "                                          input_mask  \\\n",
      "0  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                      all_segment_id  \n",
      "0  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('TrainingData/jigsaw-toxic-comment-train-processed-seqlen128.csv.zip')\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f7be1b-5aca-493f-a4c6-605f475016a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                       comment_text     toxic  \\\n",
      "0  59848  This is so cool. It's like, 'would you want yo...  0.000000   \n",
      "1  59849  Thank you!! This would make my life a lot less...  0.000000   \n",
      "2  59852  This is such an urgent design problem; kudos t...  0.000000   \n",
      "3  59855  Is this something I'll be able to install on m...  0.000000   \n",
      "4  59856               haha you guys are a bunch of losers.  0.893617   \n",
      "\n",
      "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
      "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
      "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
      "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
      "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
      "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
      "\n",
      "   ...  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  \\\n",
      "0  ...    0    0      0         0              0.0                         0   \n",
      "1  ...    0    0      0         0              0.0                         0   \n",
      "2  ...    0    0      0         0              0.0                         0   \n",
      "3  ...    0    0      0         0              0.0                         0   \n",
      "4  ...    0    0      1         0              0.0                         4   \n",
      "\n",
      "   toxicity_annotator_count  \\\n",
      "0                         4   \n",
      "1                         4   \n",
      "2                         4   \n",
      "3                         4   \n",
      "4                        47   \n",
      "\n",
      "                                      input_word_ids  \\\n",
      "0  (101, 10747, 10124, 10380, 67420, 119, 10377, ...   \n",
      "1  (101, 91327, 13028, 106, 106, 10747, 10894, 13...   \n",
      "2  (101, 10747, 10124, 11049, 10151, 10399, 22500...   \n",
      "3  (101, 12034, 10531, 26133, 146, 112, 22469, 10...   \n",
      "4  (101, 10228, 10921, 13028, 75980, 12682, 10301...   \n",
      "\n",
      "                                          input_mask  \\\n",
      "0  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                      all_segment_id  \n",
      "0  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data_unintended = pd.read_csv('TrainingData/jigsaw-unintended-bias-train-processed-seqlen128.csv.zip')\n",
    "print(train_data_unintended.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c1f57f-6861-4368-9538-53671d05a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_data['all_segment_id'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899c156b-641a-4500-bfb6-4e4066c3032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor([int(x) for x in self.df.iloc[idx]['input_word_ids'].strip('()').split(', ')]),\n",
    "            'attention_mask': torch.tensor([int(x) for x in self.df.iloc[idx]['input_mask'].strip('()').split(', ')]),\n",
    "            'token_type_ids': torch.tensor([int(x) for x in self.df.iloc[idx]['all_segment_id'].strip('()').split(', ')])\n",
    "        }\n",
    "        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        labels = [self.df.iloc[idx][col] for col in label_columns]\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0de515-2a86-4758-ab2d-aa9677fdbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = ToxicCommentsDataset(train_data)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2fa8dcb-01de-458e-91e9-eba73d082587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef56a030-0037-4d2e-acb8-3166b5d13af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shelger/miniconda3/envs/langchain/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55b03f-9a9a-487c-b7d5-3002c3facd8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('TrainingData/validation-processed-seqlen128.csv.zip')\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f90881c-ffea-4feb-9ce2-ab6ad6d39a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7131073474884033\n",
      "Loss: 0.703738808631897\n",
      "Loss: 0.7008733749389648\n",
      "Loss: 0.6925168037414551\n",
      "Loss: 0.6849724650382996\n",
      "Loss: 0.6840934157371521\n",
      "Loss: 0.6771616339683533\n",
      "Loss: 0.6697148680686951\n",
      "Loss: 0.6653908491134644\n",
      "Loss: 0.6618555784225464\n",
      "Loss: 0.6523333787918091\n",
      "Loss: 0.6482284069061279\n",
      "Loss: 0.6431896090507507\n",
      "Loss: 0.6364555358886719\n",
      "Loss: 0.6320183277130127\n",
      "Loss: 0.6240700483322144\n",
      "Loss: 0.62134850025177\n",
      "Loss: 0.6148040294647217\n",
      "Loss: 0.6131453514099121\n",
      "Loss: 0.6045658588409424\n",
      "Loss: 0.6000387072563171\n",
      "Loss: 0.5931602716445923\n",
      "Loss: 0.5878549814224243\n",
      "Loss: 0.5890147686004639\n",
      "Loss: 0.5776566863059998\n",
      "Loss: 0.5751050710678101\n",
      "Loss: 0.5696629881858826\n",
      "Loss: 0.5635476112365723\n",
      "Loss: 0.5607060194015503\n",
      "Loss: 0.5568093061447144\n",
      "Loss: 0.5506417155265808\n",
      "Loss: 0.5474492311477661\n",
      "Loss: 0.5433719158172607\n",
      "Loss: 0.5375375151634216\n",
      "Loss: 0.5296816229820251\n",
      "Loss: 0.5274196267127991\n",
      "Loss: 0.5269730091094971\n",
      "Loss: 0.5220991373062134\n",
      "Loss: 0.5094466209411621\n",
      "Loss: 0.5126747488975525\n",
      "Loss: 0.5120618343353271\n",
      "Loss: 0.5024192333221436\n",
      "Loss: 0.49803417921066284\n",
      "Loss: 0.497518390417099\n",
      "Loss: 0.48967427015304565\n",
      "Loss: 0.486713171005249\n",
      "Loss: 0.48176178336143494\n",
      "Loss: 0.48092639446258545\n",
      "Loss: 0.4796280264854431\n",
      "Loss: 0.4722013473510742\n",
      "Loss: 0.47446298599243164\n",
      "Loss: 0.46639561653137207\n",
      "Loss: 0.46418970823287964\n",
      "Loss: 0.4552896022796631\n",
      "Loss: 0.455817312002182\n",
      "Loss: 0.4532391428947449\n",
      "Loss: 0.4533846080303192\n",
      "Loss: 0.4442821443080902\n",
      "Loss: 0.43512076139450073\n",
      "Loss: 0.4375264346599579\n",
      "Loss: 0.43375295400619507\n",
      "Loss: 0.4272751808166504\n",
      "Loss: 0.43242257833480835\n",
      "Loss: 0.42667412757873535\n",
      "Loss: 0.41766607761383057\n",
      "Loss: 0.41688674688339233\n",
      "Loss: 0.41584378480911255\n",
      "Loss: 0.41931259632110596\n",
      "Loss: 0.41064196825027466\n",
      "Loss: 0.4099041819572449\n",
      "Loss: 0.4036538600921631\n",
      "Loss: 0.3964870870113373\n",
      "Loss: 0.3991818428039551\n",
      "Loss: 0.38991305232048035\n",
      "Loss: 0.38910603523254395\n",
      "Loss: 0.39249515533447266\n",
      "Loss: 0.3829716145992279\n",
      "Loss: 0.38498207926750183\n",
      "Loss: 0.3742082715034485\n",
      "Loss: 0.37189310789108276\n",
      "Loss: 0.3642676770687103\n",
      "Loss: 0.3743746280670166\n",
      "Loss: 0.38176649808883667\n",
      "Loss: 0.3674989640712738\n",
      "Loss: 0.37392061948776245\n",
      "Loss: 0.3697812557220459\n",
      "Loss: 0.3674338757991791\n",
      "Loss: 0.36279404163360596\n",
      "Loss: 0.35856732726097107\n",
      "Loss: 0.3554435670375824\n",
      "Loss: 0.34844571352005005\n",
      "Loss: 0.3436620235443115\n",
      "Loss: 0.3572744131088257\n",
      "Loss: 0.34700530767440796\n",
      "Loss: 0.34845638275146484\n",
      "Loss: 0.3456559181213379\n",
      "Loss: 0.3504103422164917\n",
      "Loss: 0.33709490299224854\n",
      "Loss: 0.34899410605430603\n",
      "Loss: 0.3339938819408417\n",
      "Loss: 0.33825576305389404\n",
      "Loss: 0.3237929940223694\n",
      "Loss: 0.3287816047668457\n",
      "Loss: 0.3250696063041687\n",
      "Loss: 0.3191276490688324\n",
      "Loss: 0.31980252265930176\n",
      "Loss: 0.3247205913066864\n",
      "Loss: 0.3271825909614563\n",
      "Loss: 0.31609249114990234\n",
      "Loss: 0.3058408498764038\n",
      "Loss: 0.3177433907985687\n",
      "Loss: 0.3152729272842407\n",
      "Loss: 0.31245729327201843\n",
      "Loss: 0.30841684341430664\n",
      "Loss: 0.30684441328048706\n",
      "Loss: 0.30282238125801086\n",
      "Loss: 0.29196733236312866\n",
      "Loss: 0.2862115502357483\n",
      "Loss: 0.3031521737575531\n",
      "Loss: 0.29337143898010254\n",
      "Loss: 0.3010718822479248\n",
      "Loss: 0.2906600534915924\n",
      "Loss: 0.3010576665401459\n",
      "Loss: 0.2893349230289459\n",
      "Loss: 0.3022595942020416\n",
      "Loss: 0.29978734254837036\n",
      "Loss: 0.28894922137260437\n",
      "Loss: 0.27105283737182617\n",
      "Loss: 0.27733132243156433\n",
      "Loss: 0.28264129161834717\n",
      "Loss: 0.28581923246383667\n",
      "Loss: 0.27634304761886597\n",
      "Loss: 0.2920389175415039\n",
      "Loss: 0.2845635414123535\n",
      "Loss: 0.2843184173107147\n",
      "Loss: 0.2917534112930298\n",
      "Loss: 0.2671947479248047\n",
      "Loss: 0.27490389347076416\n",
      "Loss: 0.2647915482521057\n",
      "Loss: 0.2824890911579132\n",
      "Loss: 0.26369112730026245\n",
      "Loss: 0.28777965903282166\n",
      "Loss: 0.26058679819107056\n",
      "Loss: 0.27520042657852173\n",
      "Loss: 0.2769584357738495\n",
      "Loss: 0.2628670036792755\n",
      "Loss: 0.271206796169281\n",
      "Loss: 0.26609769463539124\n",
      "Loss: 0.271046906709671\n",
      "Loss: 0.2663261890411377\n",
      "Loss: 0.2491084337234497\n",
      "Loss: 0.2494596242904663\n",
      "Loss: 0.2629445493221283\n",
      "Loss: 0.26501381397247314\n",
      "Loss: 0.252092570066452\n",
      "Loss: 0.2607685327529907\n",
      "Loss: 0.2636983096599579\n",
      "Loss: 0.25162526965141296\n",
      "Loss: 0.24725595116615295\n",
      "Loss: 0.2537180185317993\n",
      "Loss: 0.271451473236084\n",
      "Loss: 0.258801132440567\n",
      "Loss: 0.2591504454612732\n",
      "Loss: 0.2498626559972763\n",
      "Loss: 0.24936477839946747\n",
      "Loss: 0.2566119432449341\n",
      "Loss: 0.2574990689754486\n",
      "Loss: 0.22796188294887543\n",
      "Loss: 0.23722445964813232\n",
      "Loss: 0.24675600230693817\n",
      "Loss: 0.23121726512908936\n",
      "Loss: 0.23815545439720154\n",
      "Loss: 0.24325202405452728\n",
      "Loss: 0.22539347410202026\n",
      "Loss: 0.24406787753105164\n",
      "Loss: 0.24058517813682556\n",
      "Loss: 0.23709416389465332\n",
      "Loss: 0.24425804615020752\n",
      "Loss: 0.23176631331443787\n",
      "Loss: 0.22553949058055878\n",
      "Loss: 0.23707795143127441\n",
      "Loss: 0.24123601615428925\n",
      "Loss: 0.23400567471981049\n",
      "Loss: 0.2331583946943283\n",
      "Loss: 0.23467180132865906\n",
      "Loss: 0.22996735572814941\n",
      "Loss: 0.23798462748527527\n",
      "Loss: 0.2342832088470459\n",
      "Loss: 0.22161388397216797\n",
      "Loss: 0.21653661131858826\n",
      "Loss: 0.22613131999969482\n",
      "Loss: 0.2232256382703781\n",
      "Loss: 0.2303403913974762\n",
      "Loss: 0.21608904004096985\n",
      "Loss: 0.21997970342636108\n",
      "Loss: 0.2047397792339325\n",
      "Loss: 0.22683069109916687\n",
      "Loss: 0.2094108760356903\n",
      "Loss: 0.2208537757396698\n",
      "Loss: 0.21668612957000732\n",
      "Loss: 0.24280211329460144\n",
      "Loss: 0.22208234667778015\n",
      "Loss: 0.2067103385925293\n",
      "Loss: 0.22268474102020264\n",
      "Loss: 0.22731348872184753\n",
      "Loss: 0.2151103913784027\n",
      "Loss: 0.20002895593643188\n",
      "Loss: 0.21419355273246765\n",
      "Loss: 0.20885419845581055\n",
      "Loss: 0.21665088832378387\n",
      "Loss: 0.20828849077224731\n",
      "Loss: 0.21278434991836548\n",
      "Loss: 0.24170967936515808\n",
      "Loss: 0.2077135443687439\n",
      "Loss: 0.21025213599205017\n",
      "Loss: 0.19857081770896912\n",
      "Loss: 0.22008274495601654\n",
      "Loss: 0.20896707475185394\n",
      "Loss: 0.20710085332393646\n",
      "Loss: 0.20373181998729706\n",
      "Loss: 0.20840148627758026\n",
      "Loss: 0.21562735736370087\n",
      "Loss: 0.21334418654441833\n",
      "Loss: 0.2099078893661499\n",
      "Loss: 0.2009115219116211\n",
      "Loss: 0.20939964056015015\n",
      "Loss: 0.21400026977062225\n",
      "Loss: 0.20606714487075806\n",
      "Loss: 0.20637273788452148\n",
      "Loss: 0.20528404414653778\n",
      "Loss: 0.22597646713256836\n",
      "Loss: 0.18384329974651337\n",
      "Loss: 0.1910559982061386\n",
      "Loss: 0.18749988079071045\n",
      "Loss: 0.20427116751670837\n",
      "Loss: 0.19320033490657806\n",
      "Loss: 0.22459106147289276\n",
      "Loss: 0.18920183181762695\n",
      "Loss: 0.19522199034690857\n",
      "Loss: 0.18368816375732422\n",
      "Loss: 0.2105575054883957\n",
      "Loss: 0.20774458348751068\n",
      "Loss: 0.1838454008102417\n",
      "Loss: 0.19439443945884705\n",
      "Loss: 0.19008490443229675\n",
      "Loss: 0.18710587918758392\n",
      "Loss: 0.20223242044448853\n",
      "Loss: 0.19779948890209198\n",
      "Loss: 0.2102932184934616\n",
      "Loss: 0.20866325497627258\n",
      "Loss: 0.1855102777481079\n",
      "Loss: 0.1920543909072876\n",
      "Loss: 0.18520542979240417\n",
      "Loss: 0.1759324073791504\n",
      "Loss: 0.18394827842712402\n",
      "Loss: 0.18872526288032532\n",
      "Loss: 0.19763407111167908\n",
      "Loss: 0.1996506154537201\n",
      "Loss: 0.20777735114097595\n",
      "Loss: 0.1929667890071869\n",
      "Loss: 0.20070809125900269\n",
      "Loss: 0.20360666513442993\n",
      "Loss: 0.1817013919353485\n",
      "Loss: 0.17841701209545135\n",
      "Loss: 0.1777624785900116\n",
      "Loss: 0.19210705161094666\n",
      "Loss: 0.21267998218536377\n",
      "Loss: 0.19178470969200134\n",
      "Loss: 0.17640356719493866\n",
      "Loss: 0.18482309579849243\n",
      "Loss: 0.18868404626846313\n",
      "Loss: 0.1987909972667694\n",
      "Loss: 0.18313990533351898\n",
      "Loss: 0.1805168092250824\n",
      "Loss: 0.19465605914592743\n",
      "Loss: 0.18883924186229706\n",
      "Loss: 0.19259855151176453\n",
      "Loss: 0.19066819548606873\n",
      "Loss: 0.18076787889003754\n",
      "Loss: 0.16669785976409912\n",
      "Loss: 0.1885085552930832\n",
      "Loss: 0.19543227553367615\n",
      "Loss: 0.19461528956890106\n",
      "Loss: 0.18709802627563477\n",
      "Loss: 0.1923588216304779\n",
      "Loss: 0.16855280101299286\n",
      "Loss: 0.170866459608078\n",
      "Loss: 0.17719869315624237\n",
      "Loss: 0.1784256398677826\n",
      "Loss: 0.17751216888427734\n",
      "Loss: 0.17247502505779266\n",
      "Loss: 0.1751406192779541\n",
      "Loss: 0.18134665489196777\n",
      "Loss: 0.1688513159751892\n",
      "Loss: 0.18807819485664368\n",
      "Loss: 0.1668759286403656\n",
      "Loss: 0.1925397515296936\n",
      "Loss: 0.16590282320976257\n",
      "Loss: 0.18280522525310516\n",
      "Loss: 0.16553053259849548\n",
      "Loss: 0.17977306246757507\n",
      "Loss: 0.1877400279045105\n",
      "Loss: 0.19191044569015503\n",
      "Loss: 0.17416922748088837\n",
      "Loss: 0.1757442206144333\n",
      "Loss: 0.17722228169441223\n",
      "Loss: 0.17388972640037537\n",
      "Loss: 0.17490169405937195\n",
      "Loss: 0.19130820035934448\n",
      "Loss: 0.16910874843597412\n",
      "Loss: 0.20600324869155884\n",
      "Loss: 0.17918488383293152\n",
      "Loss: 0.17991745471954346\n",
      "Loss: 0.18428930640220642\n",
      "Loss: 0.1683775633573532\n",
      "Loss: 0.19154702126979828\n",
      "Loss: 0.18229518830776215\n",
      "Loss: 0.16821758449077606\n",
      "Loss: 0.18830296397209167\n",
      "Loss: 0.17592772841453552\n",
      "Loss: 0.1945125311613083\n",
      "Loss: 0.1614161878824234\n",
      "Loss: 0.171839639544487\n",
      "Loss: 0.18895846605300903\n",
      "Loss: 0.17615906894207\n",
      "Loss: 0.19485029578208923\n",
      "Loss: 0.16465507447719574\n",
      "Loss: 0.16061584651470184\n",
      "Loss: 0.19527792930603027\n",
      "Loss: 0.15885746479034424\n",
      "Loss: 0.16610927879810333\n",
      "Loss: 0.17409078776836395\n",
      "Loss: 0.17347221076488495\n",
      "Loss: 0.17792247235774994\n",
      "Loss: 0.1740696281194687\n",
      "Loss: 0.16296926140785217\n",
      "Loss: 0.1641407310962677\n",
      "Loss: 0.18213459849357605\n",
      "Loss: 0.17182612419128418\n",
      "Loss: 0.1708262413740158\n",
      "Loss: 0.15091940760612488\n",
      "Loss: 0.16279533505439758\n",
      "Loss: 0.17587970197200775\n",
      "Loss: 0.14691640436649323\n",
      "Loss: 0.16306358575820923\n",
      "Loss: 0.16307349503040314\n",
      "Loss: 0.17105519771575928\n",
      "Loss: 0.14361673593521118\n",
      "Loss: 0.18331101536750793\n",
      "Loss: 0.17552487552165985\n",
      "Loss: 0.157486230134964\n",
      "Loss: 0.15569207072257996\n",
      "Loss: 0.1595492959022522\n",
      "Loss: 0.16423478722572327\n",
      "Loss: 0.15482774376869202\n",
      "Loss: 0.1663506031036377\n",
      "Loss: 0.15429335832595825\n",
      "Loss: 0.1442447006702423\n",
      "Loss: 0.15654972195625305\n",
      "Loss: 0.14305073022842407\n",
      "Loss: 0.18611028790473938\n",
      "Loss: 0.16354161500930786\n",
      "Loss: 0.1608891636133194\n",
      "Loss: 0.17515811324119568\n",
      "Loss: 0.15703582763671875\n",
      "Loss: 0.16659699380397797\n",
      "Loss: 0.1657012403011322\n",
      "Loss: 0.17915861308574677\n",
      "Loss: 0.16049546003341675\n",
      "Loss: 0.15834946930408478\n",
      "Loss: 0.16554071009159088\n",
      "Loss: 0.16227534413337708\n",
      "Loss: 0.1652633547782898\n",
      "Loss: 0.1674138903617859\n",
      "Loss: 0.16629861295223236\n",
      "Loss: 0.16198056936264038\n",
      "Loss: 0.1465865969657898\n",
      "Loss: 0.15362778306007385\n",
      "Loss: 0.16987335681915283\n",
      "Loss: 0.18501222133636475\n",
      "Loss: 0.14934022724628448\n",
      "Loss: 0.19274887442588806\n",
      "Loss: 0.18035322427749634\n",
      "Loss: 0.1689426749944687\n",
      "Loss: 0.1732167899608612\n",
      "Loss: 0.16849088668823242\n",
      "Loss: 0.14692562818527222\n",
      "Loss: 0.15588079392910004\n",
      "Loss: 0.1699393391609192\n",
      "Loss: 0.16744962334632874\n",
      "Loss: 0.1454249918460846\n",
      "Loss: 0.14866869151592255\n",
      "Loss: 0.17327091097831726\n",
      "Loss: 0.17012503743171692\n",
      "Loss: 0.16304028034210205\n",
      "Loss: 0.164091095328331\n",
      "Loss: 0.1760537028312683\n",
      "Loss: 0.15240630507469177\n",
      "Loss: 0.17585580050945282\n",
      "Loss: 0.169672891497612\n",
      "Loss: 0.15961036086082458\n",
      "Loss: 0.15142413973808289\n",
      "Loss: 0.16781267523765564\n",
      "Loss: 0.16805100440979004\n",
      "Loss: 0.1673448383808136\n",
      "Loss: 0.16035035252571106\n",
      "Loss: 0.19491639733314514\n",
      "Loss: 0.1653437465429306\n",
      "Loss: 0.15553905069828033\n",
      "Loss: 0.17130374908447266\n",
      "Loss: 0.13803595304489136\n",
      "Loss: 0.19332018494606018\n",
      "Loss: 0.1679164618253708\n",
      "Loss: 0.14201854169368744\n",
      "Loss: 0.1538371592760086\n",
      "Loss: 0.18691478669643402\n",
      "Loss: 0.1660969853401184\n",
      "Loss: 0.17824986577033997\n",
      "Loss: 0.16389396786689758\n",
      "Loss: 0.14266878366470337\n",
      "Loss: 0.1554524451494217\n",
      "Loss: 0.13732776045799255\n",
      "Loss: 0.15106138586997986\n",
      "Loss: 0.16872665286064148\n",
      "Loss: 0.14441832900047302\n",
      "Loss: 0.1390591710805893\n",
      "Loss: 0.17308466136455536\n",
      "Loss: 0.168008491396904\n",
      "Loss: 0.16472312808036804\n",
      "Loss: 0.1492851972579956\n",
      "Loss: 0.1673019528388977\n",
      "Loss: 0.15136629343032837\n",
      "Loss: 0.16307690739631653\n",
      "Loss: 0.17292486131191254\n",
      "Loss: 0.137556254863739\n",
      "Loss: 0.13754276931285858\n",
      "Loss: 0.16502588987350464\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for batch in dataloader:\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    outputs = model(**inputs)\n",
    "    # print(\"Logits shape:\", outputs.logits.shape)\n",
    "    # print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "    loss = loss_fn(outputs.logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06d2a7fd-8c70-426e-9a00-b7ca6d66d5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                       comment_text lang  toxic  \\\n",
      "0        0  Este usuario ni siquiera llega al rango de    ...   es      0   \n",
      "1        1  Il testo di questa voce pare esser scopiazzato...   it      0   \n",
      "2        2  Vale. Sólo expongo mi pasado. Todo tiempo pasa...   es      1   \n",
      "3        3  Bu maddenin alt başlığı olarak  uluslararası i...   tr      0   \n",
      "4        4  Belçika nın şehirlerinin yanında ilçe ve belde...   tr      0   \n",
      "...    ...                                                ...  ...    ...   \n",
      "7995  7995   Il fatto è che la pagina dei personaggi minor...   it      0   \n",
      "7996  7996  El imbesil ete dela luna no se entera ni ostia...   es      1   \n",
      "7997  7997  olum sız manyakmısınz siz adam sıze sanal yıld...   tr      1   \n",
      "7998  7998  El mapa del reinado de Alhaken esta ligerament...   es      0   \n",
      "7999  7999  lasciami la tua email per favore. ad ogni modo...   it      0   \n",
      "\n",
      "                                         input_word_ids  \\\n",
      "0     (101, 12515, 82849, 10414, 10294, 39190, 10113...   \n",
      "1     (101, 10282, 29346, 10120, 14508, 19696, 22606...   \n",
      "2     (101, 32286, 119, 101911, 11419, 27119, 10797,...   \n",
      "3     (101, 11916, 10824, 71339, 10245, 15499, 24542...   \n",
      "4     (101, 47197, 20267, 10371, 14349, 66513, 31268...   \n",
      "...                                                 ...   \n",
      "7995  (101, 10282, 16966, 262, 10262, 10109, 24400, ...   \n",
      "7996  (101, 10224, 10211, 16216, 11030, 10131, 10112...   \n",
      "7997  (101, 30668, 10465, 187, 30471, 11299, 10710, ...   \n",
      "7998  (101, 10224, 22474, 10127, 40178, 10104, 10883...   \n",
      "7999  (101, 65399, 10500, 10109, 29095, 79515, 10178...   \n",
      "\n",
      "                                             input_mask  \\\n",
      "0     (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1     (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2     (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3     (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4     (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "...                                                 ...   \n",
      "7995  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "7996  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "7997  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "7998  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "7999  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                         all_segment_id  \n",
      "0     (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1     (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2     (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3     (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4     (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                 ...  \n",
      "7995  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7996  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7997  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7998  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7999  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[8000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_data = pd.read_csv('TrainingData/validation-processed-seqlen128.csv.zip')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c6dbdc-2c59-4f8c-adee-605b4d3e1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'finetuned_bert.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30d97f7e-a753-41cc-b5b5-eeffef9330bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'lang', 'toxic', 'input_word_ids', 'input_mask',\n",
      "       'all_segment_id'],\n",
      "      dtype='object')\n",
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate', 'input_word_ids', 'input_mask',\n",
      "       'all_segment_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('model_state_dict.pth'))\n",
    "# here i am going to use validation model to check the accuracy of my model:\n",
    "print(validation_data.columns)\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37052c3a-084b-4fae-9bd1-7f8b2bd6fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor([int(x) for x in self.df.iloc[idx]['input_word_ids'].strip('()').split(', ')]),\n",
    "            'attention_mask': torch.tensor([int(x) for x in self.df.iloc[idx]['input_mask'].strip('()').split(', ')]),\n",
    "            'token_type_ids': torch.tensor([int(x) for x in self.df.iloc[idx]['all_segment_id'].strip('()').split(', ')])\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "887d4801-1e40-4a1c-b753-5e68aaed3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestToxicCommentsDataset(validation_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97fde1cd-764f-4591-87a7-8f6297ee3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def evaluate_model(threshold=0.5):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = validation_data['toxic'].values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            print(\"batch working\")\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.sigmoid(logits).cpu().numpy()[:, 0]\n",
    "            all_predictions.extend(predictions)\n",
    "    \n",
    "    predicted_labels = (np.array(all_predictions) > threshold).astype(int)\n",
    "    \n",
    "    correct_predictions = (predicted_labels == all_labels).sum()\n",
    "    accuracy = correct_predictions / len(all_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e41b0bb3-79a8-4fc6-b448-6f70bd7531ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "batch working\n",
      "Model accuracy on test data: 0.8462\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model()\n",
    "print(f\"Model accuracy on test data: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937f856-991c-4208-9991-cbb2f7d0e951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
